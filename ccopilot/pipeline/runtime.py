"""Runtime entry point for the CourseGen pipeline."""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List

from apps.orchestrator.codeact_registry import build_default_registry
from apps.orchestrator.scientific_evaluator import ScientificEvaluator
from ccopilot.core.provenance import ProvenanceEvent

from .context import PipelineContext


@dataclass(slots=True)
class PipelineRunArtifacts:
    """Paths generated by the stub pipeline."""

    course_plan: Path
    lecture: Path
    eval_report: Path
    provenance: Path
    manifest: Path
    highlights: Path | None = None
    highlight_source: str | None = None
    use_world_model: bool = True
    teacher_trace: Path | None = None
    notebook_exports: List[Dict[str, Any]] | None = None
    notebook_export_summary: Dict[str, Any] | None = None
    scientific_metrics: Dict[str, Any] | None = None
    scientific_metrics_path: Path | None = None


def run_pipeline(ctx: PipelineContext, *, dry_run: bool = False) -> PipelineRunArtifacts | None:
    """Execute the pipeline (validate dataset â†’ orchestrator run)."""

    ctx.paths.ensure_directories()

    dataset = _load_dataset(ctx)
    dataset_summary = _summarize_dataset(dataset)
    world_model_store = ctx.config.world_model.sqlite_path
    os.environ["WORLD_MODEL_STORE"] = str(world_model_store)
    snapshot_exists = world_model_store.exists()

    ctx.provenance.log(
        ProvenanceEvent(
            stage="bootstrap",
            message="Pipeline context initialized",
            agent="ccopilot.pipeline",
            payload={
                "repo_root": str(ctx.paths.repo_root),
                "output_dir": str(ctx.paths.output_dir),
                "ablations": ctx.ablations.describe(),
                "dataset_summary": dataset_summary,
                "world_model_store": str(world_model_store),
                "world_model_store_exists": snapshot_exists,
            },
        )
    )

    if ctx.ablations.use_world_model and not snapshot_exists:
        ctx.provenance.log(
            ProvenanceEvent(
                stage="warning",
                message="World-model snapshot missing; run scripts/ingest_handcrafted.py",
                agent="ccopilot.pipeline",
                payload={"expected_path": str(world_model_store)},
            )
        )

    if dry_run:
        ctx.provenance.log(
            ProvenanceEvent(
                stage="dry_run",
                message="Dry run requested; dataset validated without emitting artifacts",
                agent="ccopilot.pipeline",
                payload={"dataset_summary": dataset_summary},
            )
        )
        return None

    from apps.orchestrator import TeacherOrchestrator

    codeact_registry = build_default_registry(dspy_handles=ctx.dspy_handles)
    orchestrator = TeacherOrchestrator(ctx, codeact_registry=codeact_registry)
    orch_artifacts = orchestrator.run_coursegen(
        dataset_summary=dataset_summary,
        world_model_store=world_model_store,
        snapshot_exists=snapshot_exists,
        codeact_registry=codeact_registry,
    )

    ctx.provenance.log(
        ProvenanceEvent(
            stage="artifacts",
            message="Teacher orchestrator generated artifacts",
            agent="ccopilot.pipeline",
            payload={
                "course_plan": str(orch_artifacts.course_plan),
                "lecture": str(orch_artifacts.lecture),
                "eval_report": str(orch_artifacts.eval_report),
                "manifest": str(orch_artifacts.manifest),
                "highlights": str(orch_artifacts.highlights) if orch_artifacts.highlights else None,
            },
        )
    )

    scientific_metrics = _evaluate_scientific_metrics(ctx, orch_artifacts)
    scientific_metrics_path: Path | None = None
    if scientific_metrics:
        scientific_metrics_path = _emit_scientific_metrics_artifact(
            orch_artifacts.manifest,
            scientific_metrics,
            ctx,
        )
        _embed_scientific_metrics(
            orch_artifacts.manifest,
            scientific_metrics,
            scientific_metrics_path,
            ctx,
        )

    return PipelineRunArtifacts(
        course_plan=orch_artifacts.course_plan,
        lecture=orch_artifacts.lecture,
        eval_report=orch_artifacts.eval_report,
        provenance=orch_artifacts.provenance,
        manifest=orch_artifacts.manifest,
        highlights=orch_artifacts.highlights,
        highlight_source=orch_artifacts.highlight_source,
        use_world_model=ctx.ablations.use_world_model,
        teacher_trace=orch_artifacts.teacher_trace,
        notebook_exports=orch_artifacts.notebook_exports,
        notebook_export_summary=orch_artifacts.notebook_export_summary,
        scientific_metrics=scientific_metrics,
        scientific_metrics_path=scientific_metrics_path,
    )


def _load_dataset(ctx: PipelineContext):
    from scripts.handcrafted_loader import load_dataset, validate_dataset  # type: ignore

    dataset = load_dataset(ctx.config.world_model.dataset_dir)
    errors, warnings = validate_dataset(dataset)
    for warning in warnings:
        ctx.provenance.log(
            ProvenanceEvent(
                stage="dataset_warning",
                message=warning,
                agent="ccopilot.pipeline",
            )
        )
    if errors:
        raise RuntimeError(f"Dataset validation failed: {errors}")
    return dataset


def _summarize_dataset(dataset) -> Dict[str, Any]:
    taxonomy = dataset.taxonomy if isinstance(dataset.taxonomy, dict) else {}
    domains: List[str] = []
    for domain in taxonomy.get("domains", []):
        title = domain.get("title") or domain.get("id")
        if title:
            domains.append(title)
        if len(domains) == 3:
            break
    return {
        "concept_count": len(getattr(dataset, "concepts", {})),
        "paper_count": len(getattr(dataset, "papers", [])),
        "timeline_count": len(getattr(dataset, "timeline", [])),
        "quiz_count": len(getattr(dataset, "quiz_bank", [])),
        "top_domains": domains,
    }


def _evaluate_scientific_metrics(ctx: PipelineContext, artifacts: Any) -> Dict[str, Any] | None:
    """Run the scientific evaluator against the latest artifacts."""

    course_plan = getattr(artifacts, "course_plan", None)
    lecture = getattr(artifacts, "lecture", None)
    if course_plan is None or lecture is None:
        return None

    science_cfg = ctx.science_config
    if not _science_module_enabled(science_cfg):
        return None

    evaluator = ScientificEvaluator(config=science_cfg)
    objectives = list(getattr(ctx.config.course, "learning_objectives", []) or [])

    try:
        metrics = evaluator.evaluate_course(
            course_plan_path=course_plan,
            lecture_paths=[lecture],
            learning_objectives=objectives,
        )
    except FileNotFoundError:
        return None
    except Exception as exc:  # pragma: no cover - defensive logging
        ctx.provenance.log(
            ProvenanceEvent(
                stage="warning",
                message="Scientific evaluator failed; metrics omitted",
                agent="ccopilot.pipeline",
                payload={"error": str(exc), "course_plan": str(course_plan), "lecture": str(lecture)},
            )
        )
        return None

    return metrics.to_dict() if metrics else None


def _emit_scientific_metrics_artifact(
    manifest_path: Path,
    metrics: Dict[str, Any],
    ctx: PipelineContext,
) -> Path | None:
    """Write scientific metrics to a standalone artifact file."""

    if not manifest_path.parent.exists():
        manifest_path.parent.mkdir(parents=True, exist_ok=True)

    base_name = manifest_path.name
    if base_name.endswith("-manifest.json"):
        target_name = base_name[: -len("-manifest.json")] + "-science.json"
    else:
        target_name = manifest_path.stem + "-science.json"
    artifact_path = manifest_path.with_name(target_name)

    payload = {
        "agent": "scientific_evaluator",
        "metrics": metrics,
    }
    artifact_path.write_text(json.dumps(payload, indent=2) + "\n", encoding="utf-8")
    ctx.provenance.log(
        ProvenanceEvent(
            stage="science_metrics",
            message="Scientific evaluator metrics saved",
            agent="ccopilot.pipeline",
            payload={"artifact": str(artifact_path)},
        )
    )
    return artifact_path


def _embed_scientific_metrics(
    manifest_path: Path,
    metrics: Dict[str, Any],
    artifact_path: Path | None,
    ctx: PipelineContext,
) -> None:
    """Persist scientific metrics alongside the manifest for downstream tooling."""

    if not manifest_path.exists():
        return

    try:
        manifest = json.loads(manifest_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:  # pragma: no cover - defensive
        ctx.provenance.log(
            ProvenanceEvent(
                stage="warning",
                message="Unable to inject scientific metrics; manifest unreadable",
                agent="ccopilot.pipeline",
                payload={"manifest": str(manifest_path), "error": str(exc)},
            )
        )
        return

    manifest["scientific_metrics"] = metrics
    if artifact_path is not None:
        manifest["scientific_metrics_artifact"] = str(artifact_path)
    manifest_path.write_text(json.dumps(manifest, indent=2) + "\n", encoding="utf-8")


def _science_module_enabled(config: Dict[str, Any] | None) -> bool:
    if not config:
        return True
    enabled = config.get("enabled")
    if isinstance(enabled, bool):
        return enabled
    eval_block = config.get("evaluation_metrics")
    if isinstance(eval_block, dict):
        eval_enabled = eval_block.get("enabled")
        if isinstance(eval_enabled, bool):
            return eval_enabled
    return True
